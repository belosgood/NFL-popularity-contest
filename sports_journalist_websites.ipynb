{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the driver for use with Selenium (Web Scraping)\n",
    "\n",
    "fp = webdriver.FirefoxProfile()\n",
    "fp.set_preference(\"browser.startup.homepage_override.mstone\", \"ignore\") #Avoid startup screen\n",
    "fp.set_preference(\"startup.homepage_welcome_url.additional\",  \"about:blank\")\n",
    "\n",
    "# Change the executable path to the location of your local gecko driver\n",
    "driver = webdriver.Firefox(firefox_profile=fp, executable_path=r\"C:\\Users\\Makarand\\Anaconda3\\Lib\\site-packages\\selenium\\webdriver\\firefox\\geckodriver.exe\")\n",
    "driver.set_window_size(1120, 550)\n",
    "driver.set_page_load_timeout(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Scraping the Sports Illustrated Website\n",
    "\n",
    "def SI_NFLscrape(nfl_info, query, driver):\n",
    "    page_url = \"https://www.si.com/nfl/\" + query\n",
    "\n",
    "    response = requests.get(page_url)\n",
    "    soup = bsoup(response.text, 'lxml')\n",
    "    si_url = re.compile(\"/nfl/\"+ query+\"/\\w+\")\n",
    "\n",
    "    for posting in set(soup.find_all('a')):\n",
    "        article_contents = \"\"\n",
    "        article_url = posting.get('href')\n",
    "        if si_url.match(str(article_url)):\n",
    "            article_url = \"https://www.si.com\" + article_url\n",
    "            #article_response = requests.get(article_url)\n",
    "            #article_soup = bsoup(response.text, 'lxml')\n",
    "            #article_title = article_soup.find(\"div\", {\"class\": \"story\"}).h1.text\n",
    "            #article_author = article_soup.find_all('p', {\"class\":\"skpzhulc sk3x58id skh9fqbk\"})\n",
    "\n",
    "            try:\n",
    "                driver.get(article_url)\n",
    "                article_title = driver.find_element_by_css_selector('h1').text\n",
    "            except: \n",
    "                continue\n",
    "            texts = driver.find_elements_by_css_selector('p')\n",
    "            for text in texts:\n",
    "                try:\n",
    "                    if text.text != \"Read More\":\n",
    "                        article_contents += text.text\n",
    "                except:\n",
    "                    continue  \n",
    "\n",
    "            #for text in texts:\n",
    "            #    if text.text != \"Read More\": #and text.text not in [author.text for author in article_author]:\n",
    "            #        article_contents += text.text        \n",
    "            nfl_info[\"Team\"].append(query)\n",
    "            nfl_info[\"Article URL\"].append(article_url)\n",
    "            nfl_info[\"Article Title\"].append(article_title)\n",
    "            #nfl_info[\"Article Author\"].append(article_author[0])\n",
    "            nfl_info[\"Text\"].append(article_contents)           \n",
    "    \n",
    "    return nfl_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping the website for ESPN\n",
    "\n",
    "def ESPN_NFLscrape(nfl_info, query, team_dict):\n",
    "    page_url = \"https://www.espn.com/blog/\" + query\n",
    "\n",
    "    response = requests.get(page_url)\n",
    "    soup = bsoup(response.text, 'lxml')\n",
    "\n",
    "    article_contents = \"\"\n",
    "    article_url = page_url\n",
    "    article_title = soup.find(\"header\", {\"class\": \"article-header\"}).text\n",
    "    #article_author = article_soup.find_all('div', {\"class\":\"author has-bio\"})\n",
    "    texts = soup.find_all('p')\n",
    "    \n",
    "    for text in texts:\n",
    "        article_contents += text.text        \n",
    "    nfl_info[\"Team\"].append(team_dict[query])\n",
    "    nfl_info[\"Article URL\"].append(article_url)\n",
    "    nfl_info[\"Article Title\"].append(article_title)\n",
    "    #nfl_info[\"Article Author\"].append(article_author[0])\n",
    "    nfl_info[\"Text\"].append(article_contents)           \n",
    "\n",
    "    return nfl_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping the SBNation website\n",
    "\n",
    "def SBNation_NFLscrape(nfl_info, query, team_dict, driver):\n",
    "    page_url = query\n",
    "\n",
    "    response = requests.get(page_url)\n",
    "    soup = bsoup(response.text, 'lxml')\n",
    "    sb_url = re.compile(query+\"\\w+\")\n",
    "\n",
    "    for posting in set(soup.find_all('a')):\n",
    "        article_contents = \"\"\n",
    "        article_url = posting.get('href')\n",
    "        if sb_url.match(str(article_url)):\n",
    "            #print(article_url)\n",
    "            #article_response = requests.get(article_url)\n",
    "            #article_soup = bsoup(response.text, 'lxml')\n",
    "            #print(article_soup.prettify())\n",
    "            \n",
    "            #article_title = article_soup.find(\"a\", {\"href\":article_url}).text\n",
    "            #article_author = article_soup.find_all('p', {\"class\":\"skpzhulc sk3x58id skh9fqbk\"})\n",
    "            #texts = article_soup.find_all('p')\n",
    "            try:\n",
    "                driver.get(article_url)\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "                article_title = driver.find_element_by_class_name('m-entry__title').text\n",
    "            except:\n",
    "                try:\n",
    "                    article_title = driver.find_element_by_class_name('c-page-title').text\n",
    "                except:\n",
    "                    try:\n",
    "                        article_title = driver.find_element_by_class_name('p-page-title').text\n",
    "                    except:\n",
    "                        continue\n",
    "            #article_author = driver.find('p', {\"class\":\"skpzhulc sk3x58id skh9fqbk\"})\n",
    "            texts = driver.find_elements_by_css_selector('p')\n",
    "\n",
    "            for text in texts:\n",
    "                try:\n",
    "                    article_contents += text.text\n",
    "                except:\n",
    "                    continue            \n",
    "            nfl_info[\"Team\"].append(team_dict[query])\n",
    "            nfl_info[\"Article URL\"].append(article_url)\n",
    "            nfl_info[\"Article Title\"].append(article_title)\n",
    "            ##nfl_info[\"Article Author\"].append(article_author[0])\n",
    "            nfl_info[\"Text\"].append(article_contents)          \n",
    "\n",
    "    return nfl_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the queries for each team and scrape the data for ESPN\n",
    "\n",
    "espn_queries = [\"las-vegas-raiders\", \"kansas-city-chiefs\", \"chicago-bears\", \"green-bay-packers\", \"dallas-cowboys\", \"philadelphia-eagles\"]\n",
    "espn_team_dict = {\"las-vegas-raiders\": \"raiders\", \"kansas-city-chiefs\":\"chiefs\", \"chicago-bears\":\"bears\", \"green-bay-packers\":\"packers\", \"dallas-cowboys\":\"cowboys\", \"philadelphia-eagles\":\"eagles\"}\n",
    "espn_nfl_info = defaultdict(list)\n",
    "\n",
    "for query in espn_queries:\n",
    "    espn_nfl_info = ESPN_NFLscrape(espn_nfl_info, query, espn_team_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the ESPN data as a DataFrame and export to csv\n",
    "\n",
    "df_espn_nfl = pd.DataFrame(espn_nfl_info)\n",
    "df_espn_nfl = df_espn_nfl.drop_duplicates(\"Text\")\n",
    "df_espn_nfl.to_csv(\"ESPN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Run the queries for each team and scrape the data from SB Nation\n",
    "\n",
    "SB_queries = [\"https://www.silverandblackpride.com/\", \"https://www.acmepackingcompany.com/\", \"https://www.bleedinggreennation.com/\", \"https://www.arrowheadpride.com/\", \"https://www.windycitygridiron.com/\", \"https://www.bloggingtheboys.com/\"]\n",
    "SB_team_dict = {\"https://www.silverandblackpride.com/\": \"raiders\", \"https://www.arrowheadpride.com/\":\"chiefs\", \"https://www.windycitygridiron.com/\":\"bears\", \"https://www.acmepackingcompany.com/\":\"packers\", \"https://www.bloggingtheboys.com/\":\"cowboys\", \"https://www.bleedinggreennation.com/\":\"eagles\"}\n",
    "sb_nfl_info = defaultdict(list)\n",
    "\n",
    "for query in SB_queries:\n",
    "    sb_nfl_info = SBNation_NFLscrape(sb_nfl_info, query, SB_team_dict, driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the SB Nation data as a DataFrame and export to csv\n",
    "\n",
    "df_sb_nfl = pd.DataFrame(sb_nfl_info)\n",
    "df_sb_nfl = df_sb_nfl.drop_duplicates(\"Text\")\n",
    "df_sb_nfl.to_csv(\"SB_Nation.csv\")"
   ]
  },
  {
   "source": [
    "## Run the queries for each team and scrape the data from Sports Illustrated\n",
    "\n",
    "si_nfl_info = defaultdict(list)\n",
    "queries = [\"packers\",\"bears\",\"eagles\",\"cowboys\",\"raiders\",\"chiefs\"]\n",
    "for query in queries:\n",
    "    page_url = \"https://www.si.com/nfl/\" + query\n",
    "    si_nfl_info = SI_NFLscrape(si_nfl_info, query, driver)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 102,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the Sports Illustrated data as a DataFrame and export to csv\n",
    "\n",
    "df_si_nfl = pd.DataFrame(si_nfl_info)\n",
    "df_si_nfl = df_si_nfl.drop_duplicates(\"Text\")\n",
    "df_si_nfl.to_csv(\"SI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the final DataFrame by concatenating all three DataFrames for ESPN, SB Nation and Sports Illustraed\n",
    "## Drop any duplicates of text\n",
    "## Then export it as sports_journalism.csv\n",
    "\n",
    "df_nfl = pd.concat([df_espn_nfl, df_sb_nfl, df_si_nfl])\n",
    "df_nfl = df_nfl.drop_duplicates(\"Text\")\n",
    "df_nfl.to_csv(\"sports_journalism.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def FiveThirtyEight_NFLscrape(nfl_info, query, team_dict):\n",
    "#    page_url = \"https://fivethirtyeight.com/?s=\" + query\n",
    "#\n",
    "#    response = requests.get(page_url)\n",
    "#    soup = bsoup(response.text, 'lxml')\n",
    "#    fte_url = re.compile(\"https://fivethirtyeight.com/features/\\w+\")\n",
    "#\n",
    "#    for posting in set(soup.find_all('a')):\n",
    "#        article_contents = \"\"\n",
    "#        article_url = posting.get('href')\n",
    "#        if fte_url.match(str(article_url)):\n",
    "#            print(article_url)\n",
    "#            try:\n",
    "#                driver.get(article_url)\n",
    "#            except:\n",
    "#                continue\n",
    "#            try:\n",
    "#                article_title = driver.find_element_by_class_name('m-entry__title').text\n",
    "#            except:\n",
    "#                try:\n",
    "#                    article_title = driver.find_element_by_class_name('c-page-title').text\n",
    "#                except:\n",
    "#                    try:\n",
    "#                        article_title = driver.find_element_by_class_name('p-page-title').text\n",
    "#                    except:\n",
    "#                        continue\n",
    "#            #article_author = driver.find('p', {\"class\":\"skpzhulc sk3x58id skh9fqbk\"})\n",
    "#            texts = driver.find_elements_by_css_selector('p')\n",
    "#\n",
    "#            for text in texts:\n",
    "#                try:\n",
    "#                    article_contents += text.text\n",
    "#                except:\n",
    "#                    continue            \n",
    "#            nfl_info[\"Team\"].append(team_dict[query])\n",
    "#            nfl_info[\"Article URL\"].append(article_url)\n",
    "#            nfl_info[\"Article Title\"].append(article_title)\n",
    "#            ##nfl_info[\"Article Author\"].append(article_author[0])\n",
    "#            nfl_info[\"Text\"].append(article_contents)             \n",
    "#    \n",
    "#    return nfl_info "
   ]
  }
 ]
}